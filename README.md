![gridworld](https://github.com/user-attachments/assets/7f19209a-9252-441d-b09c-6297b31b821d)

This is a learning project built around a deliberate constraint: to test whether a transformer can learn 2D spatial navigation without being given shortcuts. There are no intermediate rewards, no temporal embeddings, and no architectural tricks to inject memory or sequence into the policy. The goal isn't to solve the navigation problem in the most efficient way, it is to see whether the pattern-recognition machinery of a transformer can bootstrap intelligent behavior purely from end-goal feedback.<br>
The idea was to test whether a general-purpose architecture could handle a domain it wasn’t made for, not because it should be good at it, but precisely because it shouldn’t. Watching where it breaks was the point. The spatial domain was picked as a simple, controlled substrate in which intelligence could be observed as emergent generalization over structured patterns, like "walls block" or "go around". The goal was conceptual abstraction: can a transformer build internal representations that navigate a decision space, where order is not structure, as in LLMs.<br>
The project constructs a controlled curriculum to gently introduce the transformer-based agent to navigation tasks, beginning with a world building pretraining, then trivial guaranteed-success cases increasing in complexity. This scaffolding is necessary to circumvent the sparse rewards problem in RL without modifying the reward function itself. Rather than dilute the learning process with reward shaping or hand-crafted feedback, the agent is simply trained in scenarios where success likely enough to stumble onto the reward. As the training progresses through structured stages and finally through memorized obstacle configurations, the agent experiences a consistent stream of positive reinforcement associated with correct spatial understanding. The curriculum carefully maintains a solvable distribution of environments so that learning is always grounded in something achievable, while slowly nudging the agent into harder generalization.<br>
The agent doesn’t model time, each decision is made independently based only on the current grid observation. The encoder processes a 3-channel snapshot of agent, goal and walls, and a transformer encodes all spatial relationships between grid cells on the spatial relation of the agent to every other cell. The agent is timeless by design, both during training and inference.<br>
This sidesteps temporal credit assignment. When an episode is successful, every step in that trajectory is rewarded. The model learns what individual actions worked because they are part of sequences that were rewarded.<br>
What emerges is a kind of implicit planning behavior. The agent seems to figure out how to get around walls and toward goals because it has learned to encode spatial configurations in a way that makes good actions salient in the present. The result is a trained policy that looks like it's doing something clever, but is actually just reacting well in a rich spatial embedding space. It's a trained reaction function over 2D arrangements. The project isn't about building a great navigation agent, it's about probing the transformers capacity for pattern induction.<br>
What’s being trained is a statistical system for recognizing structural correlates. The agent learns to associate abstract configurations of local perception with higher expected returns, based on the patterns that emerge across many trajectories. Rather than isolating which action caused success, it internalizes an inductive bias toward action-context pairs that, in aggregate, tend to lead to reward. This is an emergent sensitivity to latent regularities in the environment. Over time, the transformer builds a representation space where certain trajectories align with reward-attracting configurations because they’re statistically downstream of good outcomes.
