![gridworld](https://github.com/user-attachments/assets/7f19209a-9252-441d-b09c-6297b31b821d)

This is a learning project built around a deliberate constraint: to test whether a transformer can learn 2D spatial navigation without being given shortcuts. There are no intermediate rewards, no temporal embeddings, and no architectural tricks to inject memory or sequence into the policy. The goal isn't to solve the navigation problem in the most efficient way, it is to see whether the pattern-recognition machinery of a transformer, when applied in a spatial domain, can bootstrap intelligent behavior purely from environmental structure and end-goal feedback.<br>
The project constructs a controlled curriculum to gently introduce the transformer-based agent to navigation tasks, beginning with a world building pretraining, then trivial guaranteed-success cases increasing in complexity. This scaffolding is necessary to circumvent the sparse rewards problem in RL without modifying the reward function itself. Rather than dilute the learning process with reward shaping or hand-crafted feedback, the agent is simply trained in scenarios where success is more likely early on. As the training progresses through structured stages and finally through memorized obstacle configurations, the agent experiences a consistent stream of positive reinforcement associated with correct spatial understanding. The curriculum carefully maintains a solvable distribution of environments so that learning is always grounded in something achievable, while slowly nudging the agent into harder generalization territory.<br>
Time is intentionally stripped out of the policy. The agent doesn’t model it, and the transformer never sees sequences, each decision is made independently based only on the current grid observation. The encoder processes a 3-channel snapshot of agent, goal and walls, and a transformer encodes all spatial relationships between grid cells on the spatial relation of the agent to every other cell. The agent is timeless by design, both during training and inference.<br>
The project sidesteps temporal credit assignment. When an episode is successful, every step in that trajectory is rewarded. The model learns what individual actions worked because they are part of sequences that are rewarded.<br>
What emerges is a kind of implicit planning behavior. The agent seems to "figure out" how to get around walls and toward goals because it has learned to encode spatial configurations in a way that makes good actions salient in the present. The result is a trained policy that looks like it's doing something clever, but is actually just reacting well in a rich spatial embedding space. It's a trained reaction function over 2D arrangements. The project isn't about building a great navigation agent, it's about probing the transformer’s capacity for spatial pattern induction in the absence of temporal scaffolding.
